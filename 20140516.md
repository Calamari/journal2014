# 16.5.2014

**home**

- 10:20 - Suche nach Dockerfähigen Servern. Vermutlich KVM Vollvirtualisierungen oder XEN (hierzu gibt es eine [Docker Installationsanleitung](http://oskarhane.com/install-docker-io-on-a-vps-under-xen/). Virtualisierungen über Virtuozzo können Docker definitiv nicht (HostEurope oder Strato), Linux VServer (OpenVZ) vermutlich auch nicht.

    - [NetCup](http://www.netcup.de/bestellen/produkt.php?produkt=568) macht nen guten Eindruck.
    - [JiffyBoxen](http://www.df.eu/de/cloud-hosting/cloud-server/leistungen/) auch, sind aber ein wenig teurer, aber flexibel in der Skalierung.
    - [Filemedia CloudVM](https://www.filemedia.de/cloudvm-ssd.php) sieht auch nett aus, vorallem der Traffic, aber weniger RAM für gleiches Geld.

- 10:50 - For having a private registry, I could use [Shipyard](https://github.com/shipyard/docker-private-registry). That could be useful.

- 11:00 - Now to do something coding: HTML-Snapshotting. There are tools for this already.

    - [grunt-html-snapshot](https://github.com/cburgdorf/grunt-html-snapshot) does this, but only for given urls, and it waits a fixed amount of time for every page. I think that can be better (crawling all links and checking them against matching function and waiting for angular to load, not fixed time?)

    Maybe I have to create an extensible crawler. One with hooks, that could be used to

    - Create a sitemap
    - Find links to further crawl
    - Save the page somewhere
    - Save the page after something says, the page is ready

    There is also a nice [Tutorial Style Blogpost](http://www.ng-newsletter.com/posts/serious-angular-seo.html) about how to achieve SEO for AngularJS.

    Technically I can use PhantomJS or [zombie.js](http://zombie.labnotes.org/) to achieve this. Or maybe use a service like [prerender.io](https://prerender.io/)?

    Prerender.io is a middleware (for node or rails) that renders HTML on demand and caches them in S3 (instead of local HTML files).

- 13:30 - Git-Repos in Dropbox are a rather fragile idea. Dropbox just had sync problems in a log of files. Also in .git directories and has resolved the problems in resetting the files to an much older state and put a copied files with some new ending beside them. Git just can't cope with that files within the .git directory and breaks. I really have to clean up there...

    My Fix for that is:

    ```bash
    find . | grep 'georgs-air.frit' | rename -f --subst " (georgs-air.fritz.box's conflicted copy 2014-05-16)" ''
    ```

- 14:00 - Back to HTML snapshotting. I try a PhantomJS bases solution. Interesting part, which makes it hard for just crawling is, that it have to search for events? Or doesn't it? Because google does not. And the pages behind a search can not be found…

- 14:50 - The phantomjs-node npm package is not able to do asynchronous page evaluations, but PhantomJS should be. It just does not wrap everything.

- 15:30 - I think I will go with a own [prerender server](https://github.com/prerender/prerender). That's MIT licensed, so I can just use it and see how it goes.

    Somehow the server doesn't work properly, the pages are empty. But I guess I will try that out, on a real server.

- 23:20 - The NetCup server could install docker (using the ansible role and installing python-pycurl)
